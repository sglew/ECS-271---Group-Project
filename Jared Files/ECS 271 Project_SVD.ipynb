{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6a1de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Code to calculate the entropy of each classifier matrix using Singualr Values. \n",
    "def compute_svd_entropy(singular_values_mat):\n",
    "    class_entropies = {int(cls): 0 for cls in range(6)}\n",
    "\n",
    "    for j in range(6):\n",
    "        singular_values = singular_values_mat[:, j]\n",
    "        # Normalize the singular values. \n",
    "        total_sum = np.sum(singular_values)\n",
    "        normalized_singular_values = singular_values / total_sum\n",
    "    \n",
    "        # Compute the (differential) entropy. \n",
    "        entropy = -np.sum(normalized_singular_values * np.log2(normalized_singular_values + 1e-12))  # Add small value to avoid log(0). \n",
    "        class_entropies[j] = entropy\n",
    "\n",
    "    # Compute average entropy to return as well. \n",
    "    avg_entropy = np.mean(list(class_entropies.values()))\n",
    "    \n",
    "    return class_entropies, avg_entropy\n",
    "\n",
    "\n",
    "def Emotion_SVD_Classification(X_train, X_test, y_train, y_test, k):\n",
    "    # Compute the rank k SVD of the training data. Do so based on class labels. \n",
    "    # 6 total class labels, so 6 different rank k SVDs. \n",
    "    def Rank_k_SVD(X_train, y_train, k):\n",
    "        unique_emotions = np.unique(y_train) # Check which emotions are represented in the data. \n",
    "        n_classes = len(unique_emotions)\n",
    "\n",
    "        U = np.zeros((X_train.shape[1], k, n_classes))  # For storing U (left singular vectors for each emotion). \n",
    "        S = np.zeros((k, n_classes))  # For storing singular values of eavh emotion. \n",
    "        Vt = np.zeros((k, X_train.shape[1], n_classes))  # For storing Vt (right singular vectors for each emotion). \n",
    "\n",
    "        # Loop over each emotion (class): \n",
    "        for j, emotion in enumerate(unique_emotions):\n",
    "            # Select the indices corresponding to the current emotion\n",
    "            emotion_indices = np.where(y_train == emotion)[0]\n",
    "            emotion_data = X_train[emotion_indices]\n",
    "\n",
    "            # Perform a Truncated SVD for the current emotion data (fast method): \n",
    "            svd = TruncatedSVD(n_components=k)\n",
    "            svd.fit(emotion_data)\n",
    "        \n",
    "            U[:, :, j] = svd.components_.T  # Left singular vectors (rows of U). \n",
    "            S[:, j] = svd.singular_values_  # Singular values (diagonal entries of Sigma). \n",
    "            Vt[:, :, j] = svd.components_  # Right singular vectors (Vt). \n",
    "        train_u = U; singular_values_mat = S # Only need the left singular vectors for reconstruction and singular values for entropy. \n",
    "        return train_u, singular_values_mat\n",
    "\n",
    "    def Rank_k_expansion_coeff_mat(X_test, train_u):\n",
    "        # The rank k can be found by measuring the size of the 2nd dimension of the parameter matrix train_u. \n",
    "        k = train_u.shape[1]\n",
    "        n_classes = train_u.shape[2]\n",
    "\n",
    "        # Initialize the matrix for the expansion coefficients. (k*n_samples)*num_classes\n",
    "        n_samples = X_test.shape[0]  # number of test samples\n",
    "        test_svd = np.zeros((k, n_samples, n_classes))\n",
    "\n",
    "        # Compute the expansion coefficients for each emotion class. \n",
    "        for j in range(n_classes):\n",
    "            # Project the test patterns onto the left singular vectors for this emotion class. \n",
    "            test_svd[:, :, j] = train_u[:, :, j].T @ X_test.T  # Transpose X_test to match the shape. \n",
    "        return test_svd\n",
    "\n",
    "    def svd_rank_k_error(X_test, train_u, test_svd):\n",
    "        # Initialize an array to store the results (approximation errors). \n",
    "        test_svdres = np.zeros((train_u.shape[2], X_test.shape[0]))  # (n_classes, n_samples)\n",
    "        # Reconstruct training data with each matrix from test_svd. \n",
    "        # Check columns. Store euclidean distance of the columns to each original test vector. \n",
    "        for j in range(train_u.shape[2]):\n",
    "            temp_jth_approx = train_u[:,:,j] @ test_svd[:,:,j]\n",
    "            for i in range(X_test.shape[0]):\n",
    "                distance_vector = X_test[i, :] - temp_jth_approx[:, i]\n",
    "                test_svdres[j, i] = np.linalg.norm(distance_vector, 2)\n",
    "\n",
    "        # Returns the residuals of the test_svd in relation to the original test data. \n",
    "        # Hope is that data in each class space should be reconstructed well by its associated train_u. \n",
    "        return test_svdres\n",
    "\n",
    "    def convert_emotions_to_integers(y_data, emotion_map):\n",
    "        # Easier to work with than the string labels for emotions. \n",
    "        return np.array([emotion_map[emotion] for emotion in y_data])\n",
    "\n",
    "    def compute_confusion_matrix(test_predict, y_test_int, n_classes=6):\n",
    "        # Initialize a confusion matrix. \n",
    "        test_confusion = np.zeros((n_classes, n_classes))\n",
    "    \n",
    "        # Populate the confusion matrix. \n",
    "        # Go through what the true labels are and keep track of the predictions of the entries that should have had that label. \n",
    "        # Best case would be only entries down the diagonal. That would be 100% accuracy. \n",
    "        for i in range(len(y_test_int)):\n",
    "            true_label = y_test_int[i]\n",
    "            predicted_label = int(test_predict[i])\n",
    "            test_confusion[true_label, predicted_label] += 1\n",
    "    \n",
    "        return test_confusion\n",
    "\n",
    "    # Convert emotions to integer labels for the training and test data. \n",
    "    emotion_map = {'Angry': 0, 'Disgust': 1, 'Fear': 2, 'Happy': 3, 'Neutral': 4, 'Sad': 5}\n",
    "    y_train_int = convert_emotions_to_integers(y_train, emotion_map)\n",
    "    y_test_int = convert_emotions_to_integers(y_test, emotion_map)\n",
    "\n",
    "\n",
    "    # Initialize results for rate and test_predict that \"don't make sense,\" which allows us to more easily identify where errors occur. \n",
    "    rate = -1\n",
    "    test_predict = np.zeros(X_test.shape[0])\n",
    "\n",
    "    # Compute the necessary values for our calculations. \n",
    "    train_u, singular_values_mat = Rank_k_SVD(X_train, y_train_int, k)\n",
    "    test_svd = Rank_k_expansion_coeff_mat(X_test, train_u)\n",
    "    test_svdres = svd_rank_k_error(X_test, train_u, test_svd)\n",
    "\n",
    "    # Prediction step. The minimum entry of the columns of test_svdres is the predicted label. \n",
    "    # These results would likely be different if a measure other than Euclidean disatnce was used. \n",
    "    for i in range(X_test.shape[0]):\n",
    "        test_predict[i] = np.argmin(test_svdres[:, i])\n",
    "    \n",
    "    # Confusion matrix calculation using the integer labels. \n",
    "    test_confusion = compute_confusion_matrix(test_predict, y_test_int)\n",
    "\n",
    "    num_correct_matched = np.sum(np.diag(test_confusion))\n",
    "    rate = num_correct_matched / X_test.shape[0]\n",
    "\n",
    "    # Computations to get the entropy. \n",
    "    class_entropies, avg_entropy = compute_svd_entropy(singular_values_mat)\n",
    "    \n",
    "    print(\"Average entropy:\", avg_entropy)\n",
    "    print(\"Per-class entropies:\", class_entropies)\n",
    "    \n",
    "    return rate, test_predict, test_confusion\n",
    "\n",
    "\n",
    "\n",
    "def generate_train_and_test_data(folder, train_percent=0.5):\n",
    "    # Allocate a split for the training and testing data. \n",
    "    # Note: To have the code be better, we would have separated test data into a validation set and \n",
    "    # test data, and have run the tests on the validation to pick an optimal rank k, but generally \n",
    "    # higher k will work better than lower k. \n",
    "\n",
    "    # Load all .npz files from the folder. \n",
    "    npz_files = [f for f in os.listdir(folder) if f.endswith(\".npz\")]\n",
    "\n",
    "    # Randomly shuffle the files and split into training and testing sets. \n",
    "    np.random.shuffle(npz_files)\n",
    "    # Calculate training and testing file split. \n",
    "    train_size = int(len(npz_files) * train_percent)\n",
    "    train_files = npz_files[:train_size]\n",
    "    test_files = npz_files[train_size:]\n",
    "\n",
    "    # Stuff for training data. \n",
    "    # Load the training data from the files. \n",
    "    X_train_list = []; y_train_list = []\n",
    "    for file in train_files:\n",
    "        data = np.load(f\"{folder}/{file}\", allow_pickle=True)\n",
    "        X_train_list.append(data['coords'])\n",
    "        y_train_list.append(data['emotions'])\n",
    "    # Convert to numpy arrays\n",
    "    X_train = np.concatenate(X_train_list, axis=0)\n",
    "    y_train = np.concatenate(y_train_list, axis=0)\n",
    "    # Flatten the X_train array. \n",
    "    X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "\n",
    "    # Stuff for test data. \n",
    "    # Load testing data from remaining files\n",
    "    X_test_list = []; y_test_list = []\n",
    "    for file in test_files:\n",
    "        data = np.load(f\"{folder}/{file}\", allow_pickle=True)\n",
    "        X_test_list.append(data['coords'])\n",
    "        y_test_list.append(data['emotions'])\n",
    "\n",
    "    # Convert to numpy arrays. \n",
    "    X_test = np.concatenate(X_test_list, axis=0)\n",
    "    y_test = np.concatenate(y_test_list, axis=0)\n",
    "    # Flatten the X_test array. \n",
    "    X_test = X_test.reshape(X_test.shape[0], -1)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ac04f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulls the data from the folder. Separates into training and testing data based on a desired percentage. \n",
    "folder = \"video_frames_npz\"\n",
    "X_train, X_test, y_train, y_test = generate_train_and_test_data(folder, train_percent=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c0e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average entropy: 0.6203845325485418\n",
      "Per-class entropies: {0: np.float64(0.6389778136414924), 1: np.float64(0.6152517837837952), 2: np.float64(0.6116633801958407), 3: np.float64(0.6350117373512845), 4: np.float64(0.60463989789449), 5: np.float64(0.6167625824243479)}\n",
      "\n",
      "Accuracy Rate: 0.2984742548518926\n",
      "Confusion Matrix:\n",
      " [[ 7200.  3917.  5164.  3291.  4164.  4007.]\n",
      " [ 4608. 10588.  4550.  5289.  2954.  4154.]\n",
      " [ 5729.  3559.  7877.  2872.  5003.  4996.]\n",
      " [ 2638.  3165.  2405. 13736.  2873.  3137.]\n",
      " [ 4095.  2982.  5069.  1295.  5524.  5387.]\n",
      " [ 4540.  5565.  6053.  1944.  6302.  6857.]]\n"
     ]
    }
   ],
   "source": [
    "k = 5  # Use rank 5 for SVD. \n",
    "rate, test_predict, test_confusion = Emotion_SVD_Classification(X_train, X_test, y_train, y_test, k)\n",
    "\n",
    "print(\"\\nAccuracy Rate:\", rate)\n",
    "print(\"Confusion Matrix:\\n\", test_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e35072b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average entropy: 0.9092454603026966\n",
      "Per-class entropies: {0: np.float64(0.9401354466223886), 1: np.float64(0.9219908474864223), 2: np.float64(0.8909344278311391), 3: np.float64(0.943929383029909), 4: np.float64(0.8707473664034893), 5: np.float64(0.8877352904428307)}\n",
      "\n",
      "Accuracy Rate: 0.373464600061099\n",
      "Confusion Matrix:\n",
      " [[ 8285.  3253.  5621.  2285.  4724.  3575.]\n",
      " [ 3425. 13276.  5465.  3666.  2491.  3820.]\n",
      " [ 4588.  3224. 11072.  2167.  4007.  4978.]\n",
      " [ 2778.  2693.  3709. 14739.  2265.  1770.]\n",
      " [ 3176.  1653.  3980.  1483.  8868.  5192.]\n",
      " [ 4242.  3446.  6678.  2016.  6327.  8552.]]\n"
     ]
    }
   ],
   "source": [
    "k = 25  # Use rank 25 for SVD. \n",
    "rate, test_predict, test_confusion = Emotion_SVD_Classification(X_train, X_test, y_train, y_test, k)\n",
    "\n",
    "print(\"\\nAccuracy Rate:\", rate)\n",
    "print(\"Confusion Matrix:\\n\", test_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f14c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average entropy: 0.9838690442852428\n",
      "Per-class entropies: {0: np.float64(1.0165242460159976), 1: np.float64(1.0003959298225205), 2: np.float64(0.9637729112723595), 3: np.float64(1.0208975849866897), 4: np.float64(0.9411158876307879), 5: np.float64(0.9605077059831006)}\n",
      "\n",
      "Accuracy Rate: 0.43922669448783497\n",
      "Confusion Matrix:\n",
      " [[ 9282.  1969.  4663.  2206.  5419.  4204.]\n",
      " [ 2859. 17382.  4432.  1714.  1764.  3992.]\n",
      " [ 3822.  2404. 11493.  2276.  4518.  5523.]\n",
      " [ 2421.  1986.  2674. 15954.  2885.  2034.]\n",
      " [ 2782.   914.  3489.  1010. 11897.  4260.]\n",
      " [ 3421.  3140.  6123.  1335.  7049. 10193.]]\n"
     ]
    }
   ],
   "source": [
    "k = 50  # Use rank 50 for SVD. \n",
    "rate, test_predict, test_confusion = Emotion_SVD_Classification(X_train, X_test, y_train, y_test, k)\n",
    "\n",
    "print(\"\\nAccuracy Rate:\", rate)\n",
    "print(\"Confusion Matrix:\\n\", test_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcda2ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 100  # Use rank 100 for SVD. \n",
    "rate, test_predict, test_confusion = Emotion_SVD_Classification(X_train, X_test, y_train, y_test, k)\n",
    "\n",
    "print(\"\\nAccuracy Rate:\", rate)\n",
    "print(\"Confusion Matrix:\\n\", test_confusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath",
   "language": "python",
   "name": "sage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
